# Optimization

This directory has the exercises to learn a little bit of optimization

## Before to code

Topics to learn:

What is a hyperparameter?
How and why do you normalize your input data?
What is a saddle point?
What is stochastic gradient descent?
What is mini-batch gradient descent?
What is a moving average? How do you implement it?
What is gradient descent with momentum? How do you implement it?
What is RMSProp? How do you implement it?
What is Adam optimization? How do you implement it?
What is learning rate decay? How do you implement it?
What is batch normalization? How do you implement it?

## Requirements
Numpy, Scipy, tensorflow and pycodestyle modules


```bash
pip install --user numpy==1.15
pip install --user scipy==1.3
pip install --user pycodestyle==2.5
pip install --user tensorflow==1.12
```

##Running main files
```bash
./0-main.py

```


## Author
[Paulo Morillo](https://www.linkedin.com/in/paulo-morillo-mu%C3%B1oz-191745143/)

## License
[MIT](https://choosealicense.com/licenses/mit/)